-> define a proper protocol for communication between nodes & schedulers
    -> nodes should register, and give the scheduling process all relevant info
    -> scheduler should return that its succesfully registered it
    -> we should have a execute command, from scheduler to node with the relevant task & parameters
    -> we should have a finished command, for the node to tell the scheduler that its done
    (maybe) we should have a health check command to make sure that a node is still up


-> write a proper launch script for all nodes

-> set up task parametrization
TASK_TEMPLATES = {
    "matmul": {
        "params": ["size"],  # 500, 1000, 2000, 3000
        "resource_profile": "cpu_heavy",
        "base_duration": lambda size, node: (size/1000)**2.7 * node.cpu_factor
    },
    
    "prime_calc": {
        "params": ["max_n"],  # 100000, 500000, 1000000
        "resource_profile": "cpu_heavy",
        "base_duration": lambda max_n, node: (max_n/100000) * 5 * node.cpu_factor
    },
    
    "sort_array": {
        "params": ["array_size_mb"],  # 10, 50, 100, 200
        "resource_profile": "ram_heavy",
        "base_duration": lambda size_mb, node: (size_mb/10) * 2 * node.cpu_factor,
        "ram_required": lambda size_mb: size_mb * 1.5  # overhead
    },
    
    "file_write": {
        "params": ["file_size_mb", "block_size_kb"],
        "resource_profile": "io_heavy",
        "base_duration": lambda size_mb, block, node: (size_mb/10) * node.io_factor
    },
    
    "video_transcode": {
        "params": ["resolution", "duration_s"],  # "480p", "720p", "1080p"
        "resource_profile": "mixed",
        "base_duration": lambda res, dur, node: dur * RES_MULTIPLIER[res] * node.cpu_factor,
        "ram_required": lambda res: RES_RAM[res]
    }
}


Write a sweep for the range of parameter values (discretize & loop) & build a time estimation model for each node, for each task
